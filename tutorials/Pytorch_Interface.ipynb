{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, and welcome to the Tequila PyTorch tutorial!\n",
    "\n",
    "### In this tutorial, we will demonstrate how to use the Tequila  PyTorch interface. If you do not already have the PyTorch module -- as well as a quantum chemistry backend -- this tutorial will not function.\n",
    "\n",
    "In this tutorial we will learn how to transform a Tequila `Objective` into a PyTorch `nn.Module` object, and will optimize such an objective using PyTorch. In particular, we are going to train a Quantum Autoencoder on the $\\text{H}_{2}$, sto-3g, UCC-SD ansatz. We will consider, as input data, the angles of the UCC-SD state prep circuit, calculated at different bond lengths, and as internal weights to the model, the angles of rotation gates within the autoencoder circuit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tequila as tq\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "cuda0 = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: generate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start by creating a number of molecule objects, each representing $\\text{H}_{2}$ at different bond lengths. \n",
    "### Subsequently, we extract the parameter value (there is only one) of the UCCSD ansatz, for preparing that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_lengths=np.linspace(.3,1.6,20) # our bond length, in angstrom.\n",
    "amp_arrays = []\n",
    "state_preps = []\n",
    "for i in bond_lengths:\n",
    "    # the line below initializes a tequila molecule object for H2 at a specific bond length.\n",
    "    # see the quantum chemistry tutorial for more details.\n",
    "    molecule = tq.chemistry.Molecule(geometry = \"H 0.0 0.0 0.0\\n H 0.0 0.0 {}\".format(str(i)), basis_set=\"sto-3g\")\n",
    "    amplitude = molecule.compute_amplitudes(method='ccsd') # get the state prep amplitudes\n",
    "    amp_arrays.append(np.asarray([v for v in amplitude.make_parameter_dictionary().values()]))\n",
    "    state_preps.append(molecule.make_uccsd_ansatz(trotter_steps=1,initial_amplitudes=amplitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we create the pytorch `DataSet` and `DataLoader` to load data from. To do so, we must implement our own inheritor from the basic pytorch `DataSet`  class\n",
    "\n",
    "Our `Dataset` inheritor, named `AngleData`, needs to define `__len__` and `__getitem__` methods to function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AngleData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self,data_list):\n",
    "        self.data_list=data_list\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return torch.from_numpy(self.data_list[idx]).to(torch.device('cuda:0'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now construct a `DataLoader`. We will enable batching, so that we train over random subsets over the data when we finally train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = AngleData(amp_arrays)\n",
    "my_loader = torch.utils.data.DataLoader(my_data,batch_size=4,shuffle=True) ### here's our data loader!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Create an Ansatz `Objective`.\n",
    "\n",
    "Our autoencoder ansatz must be a circuit that both constructs the hydrogen wavefunction, and then compresses it. Furthermore, the autoencoder cost function must be encoded into the objective. The cost function can be defined as a hamiltonian which projects all the non-latent, or 'trash' qubits, onto the all zero state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined state prep, encoder circuit:  \n",
      " circuit: \n",
      "X(target=(0,))\n",
      "X(target=(1,))\n",
      "Trotterized(target=(0, 1, 2, 3), control=(), angles=[(1, 0, 1, 0), (1, 0, 1, 0), Objective with 0 unique expectation values\n",
      "variables = [(1, 0, 1, 0)]\n",
      "types     = [], Objective with 0 unique expectation values\n",
      "variables = [(1, 0, 1, 0)]\n",
      "types     = []], generators=[+0.1250X(0)X(1)Y(2)X(3)-0.1250Y(0)X(1)X(2)X(3)-0.1250Y(0)Y(1)Y(2)X(3)-0.1250X(0)Y(1)X(2)X(3)+0.1250Y(0)X(1)Y(2)Y(3)+0.1250X(0)X(1)X(2)Y(3)+0.1250X(0)Y(1)Y(2)Y(3)-0.1250Y(0)Y(1)X(2)Y(3), -0.1250Y(0)Y(1)Y(2)X(3)+0.1250Y(0)X(1)Y(2)Y(3)+0.1250X(0)X(1)Y(2)X(3)+0.1250X(0)Y(1)Y(2)Y(3)-0.1250Y(0)X(1)X(2)X(3)-0.1250Y(0)Y(1)X(2)Y(3)-0.1250X(0)Y(1)X(2)X(3)+0.1250X(0)X(1)X(2)Y(3), , ])\n",
      "Rx(target=(0,), parameter=a)\n",
      "Rx(target=(1,), parameter=b)\n",
      "X(target=(3,), control=(1,))\n",
      "X(target=(2,), control=(0,))\n",
      "X(target=(1,), control=(0,))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = tq.gates.Rx('a',0) +tq.gates.Rx('b',1) +tq.gates.CNOT(1,3) +tq.gates.CNOT(0,2)+tq.gates.CNOT(0,1)\n",
    "state_prep = state_preps[0] # every member of this list is the same object; it doesn't matter which we pick.\n",
    "combined = state_prep + encoder\n",
    "print('combined state prep, encoder circuit:  \\n', combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2 autoencoder:  Objective with 1 unique expectation values\n",
      "variables = [(1, 0, 1, 0), b, a]\n",
      "types     = not compiled\n"
     ]
    }
   ],
   "source": [
    "# we decide that the 3rd and 4th qubits will be trash qubits. The hamiltonian below projects onto zero.\n",
    "hamiltonian = tq.hamiltonian.paulis.Qm(2)*tq.hamiltonian.paulis.Qm(3)\n",
    "h2_encoder = tq.ExpectationValue(U=combined,H=hamiltonian)\n",
    "print('H2 autoencoder: ', h2_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Convert the Tequila `Objective` into a PyTorch `Module`.\n",
    "\n",
    "in order for pytorch to interact with Tequila objectives, we need to build wrappers which allow the classes of pytorch and tequila to work together. For user convenience, this can be done with the class `TorchLayer`. \n",
    "This class takes an `Objective`, a dict of compilation args (optional), and a list of `Variable`s of the objective which should be treater as input (I.e, loaded from the `DataLoader`), and will treat all the remaining variables as internal weights of the quantum neural network layer that the objective constitutes. If not specified in the compile_args dictionary, random values (between 0 and $2 \\pi$) will be used for the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variable=h2_encoder.extract_variables()[0]\n",
    "#inits={'a':1.5, 'b':1.5}\n",
    "compile_args={'backend':'qulacs'} # dict. allowed keys: backend, samples, noise, device, initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchLayer(\n",
      "  Tequila TorchLayer. Represents: \n",
      "  Objective with 1 unique expectation values\n",
      "  variables = [(1, 0, 1, 0), b, a]\n",
      "  types     = not compiled \n",
      "  Current Weights: {'b': Parameter containing:\n",
      "  tensor(4.0312, requires_grad=True), 'a': Parameter containing:\n",
      "  tensor(5.6456, requires_grad=True)}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_torch_encoder = tq.ml.interface_torch.TorchLayer(h2_encoder,compile_args,input_vars=[input_variable])\n",
    "print(my_torch_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Optimize using PyTorch and plot the results.\n",
    "\n",
    "Once converted to a PyTorch layer, Tequila `Objective`'s can be optimized with the inbuilt tools of PyTorch. Do note that `TorchLayer` objects cannot be optimized with the tequila optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 0 ***\n",
      "Batched Average Loss:  0.1589804690593338\n",
      "*** Epoch 1 ***\n",
      "Batched Average Loss:  0.13979248038737174\n",
      "*** Epoch 2 ***\n",
      "Batched Average Loss:  0.12193431082165614\n",
      "*** Epoch 3 ***\n",
      "Batched Average Loss:  0.10552483691985856\n",
      "*** Epoch 4 ***\n",
      "Batched Average Loss:  0.09064034377472838\n",
      "*** Epoch 5 ***\n",
      "Batched Average Loss:  0.07730965488499583\n",
      "*** Epoch 6 ***\n",
      "Batched Average Loss:  0.065514897122055\n",
      "*** Epoch 7 ***\n",
      "Batched Average Loss:  0.055196851336978756\n",
      "*** Epoch 8 ***\n",
      "Batched Average Loss:  0.04626343966433194\n",
      "*** Epoch 9 ***\n",
      "Batched Average Loss:  0.03859953636461928\n",
      "*** Epoch 10 ***\n",
      "Batched Average Loss:  0.032077118860465145\n",
      "*** Epoch 11 ***\n",
      "Batched Average Loss:  0.026563873636689255\n",
      "*** Epoch 12 ***\n",
      "Batched Average Loss:  0.02193025827531985\n",
      "*** Epoch 13 ***\n",
      "Batched Average Loss:  0.018054284081680604\n",
      "*** Epoch 14 ***\n",
      "Batched Average Loss:  0.014824628384913252\n",
      "*** Epoch 15 ***\n",
      "Batched Average Loss:  0.012142080885633478\n",
      "*** Epoch 16 ***\n",
      "Batched Average Loss:  0.009919946422047143\n",
      "*** Epoch 17 ***\n",
      "Batched Average Loss:  0.00808352800215759\n",
      "*** Epoch 18 ***\n",
      "Batched Average Loss:  0.00656914727489224\n",
      "*** Epoch 19 ***\n",
      "Batched Average Loss:  0.005322976171478523\n",
      "*** Epoch 20 ***\n",
      "Batched Average Loss:  0.004299752115908129\n",
      "*** Epoch 21 ***\n",
      "Batched Average Loss:  0.0034615624820748024\n",
      "*** Epoch 22 ***\n",
      "Batched Average Loss:  0.002776709299954397\n",
      "*** Epoch 23 ***\n",
      "Batched Average Loss:  0.0022187456318869233\n",
      "*** Epoch 24 ***\n",
      "Batched Average Loss:  0.0017655901621152323\n",
      "*** Epoch 25 ***\n",
      "Batched Average Loss:  0.0013988438821460353\n",
      "*** Epoch 26 ***\n",
      "Batched Average Loss:  0.001103165460931678\n",
      "*** Epoch 27 ***\n",
      "Batched Average Loss:  0.0008657682312413671\n",
      "*** Epoch 28 ***\n",
      "Batched Average Loss:  0.0006760170189193379\n",
      "*** Epoch 29 ***\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(my_torch_encoder.parameters(),lr=0.01)\n",
    "loss_values = []\n",
    "for epoch in range(30):\n",
    "    print('*** Epoch {} ***'.format(epoch))\n",
    "    batch = 0\n",
    "    batched_loss = []\n",
    "    for point in my_loader:\n",
    "        batch += 1\n",
    "        optim.zero_grad()\n",
    "        loss = my_torch_encoder(point)\n",
    "        loss = loss.mean()\n",
    "        batched_loss.append(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "    bv = np.mean([l.detach().numpy() for l in batched_loss])\n",
    "    loss_values.append(bv)\n",
    "    print('Batched Average Loss: ', bv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_values, label='loss per epoch')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel('Autoencoder Loss', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This concludes our tutorial. We hope you've enjoyed it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
